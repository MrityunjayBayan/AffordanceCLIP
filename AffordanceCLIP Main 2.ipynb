{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V32FaqZ-au2R"
      },
      "source": [
        "**Installing OpenAI CLIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVHpSn6h8kWZ"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkOm9FwJbU2X"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3gXLP1dbZxT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKHP0R-gbhY3"
      },
      "source": [
        "**Load and Freeze CLIP Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoZzxYsLbzFM"
      },
      "outputs": [],
      "source": [
        "# Initialize CLIP model and the preprocess function for the images\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model, preprocess = clip.load(\"RN101\", device=device)\n",
        "\n",
        "# Freeze CLIP model parameters (no training required)\n",
        "\n",
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxeeFbbb3A2"
      },
      "source": [
        "**Loading Affordance Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N_wnnuPb6Aq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfz9lAgub8Gf"
      },
      "source": [
        "**Define Paths to Dataset Directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epkErLu_cAxA"
      },
      "outputs": [],
      "source": [
        "train_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Images'\n",
        "train_masks_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Masks'\n",
        "train_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/annotations.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BdAklghcJQG"
      },
      "outputs": [],
      "source": [
        "test_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/Images'\n",
        "test_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/annotations.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_dir = '/content/drive/My Drive/Dataset/Train_Set/Images'\n",
        "train_masks_dir = '/content/drive/My Drive/Dataset/Train_Set/Masks'\n",
        "train_annotations_path = '/content/drive/My Drive/Dataset/Train_Set/annotations.json'"
      ],
      "metadata": {
        "id": "7M6E3_Ji_Sv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa5RXr8bcKIp"
      },
      "source": [
        "**Load annotations from the JSON file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTYzBIrMcNXd"
      },
      "outputs": [],
      "source": [
        "def load_annotations(annotation_path):\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    return annotations\n",
        "\n",
        "train_annotations = load_annotations(train_annotations_path)\n",
        "test_annotations = load_annotations(test_annotations_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBhVnafMcWI_"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLhCFHbHcZt7"
      },
      "outputs": [],
      "source": [
        "class AffordanceDataset(Dataset):\n",
        "    def __init__(self, annotations, images_dir, masks_dir, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def load_mask(self, mask_path):\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform:\n",
        "            mask = self.transform(mask)\n",
        "        return mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        query = annotation['query']\n",
        "        object_type = annotation['object_type']\n",
        "        image_file = annotation['file_name']\n",
        "        mask_file = annotation['mask_file_name']\n",
        "\n",
        "        queries = query + \" \" + object_type\n",
        "\n",
        "        image_path = os.path.join(self.images_dir, query, object_type, image_file)\n",
        "        mask_path = os.path.join(self.masks_dir, query, object_type, mask_file)\n",
        "\n",
        "        image = self.load_image(image_path)\n",
        "        mask = self.load_mask(mask_path)\n",
        "\n",
        "        image = self.preprocess(image)\n",
        "\n",
        "        return image, queries, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9IzCuuDcgvu"
      },
      "source": [
        "**Train Dataset Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVX0HgIjdZ0a"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((60, 60)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = AffordanceDataset(\n",
        "    annotations=train_annotations,\n",
        "    images_dir=train_images_dir,\n",
        "    masks_dir=train_masks_dir,\n",
        "    transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phaCXt5GuCuw"
      },
      "source": [
        "**Create DataLoader for Batching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3P5Si-RuHJA"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkzBlztMvcku"
      },
      "source": [
        "**Feature Pyramid Network (FPN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZNIosGlvhPu"
      },
      "outputs": [],
      "source": [
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeaturePyramidNetwork, self).__init__()\n",
        "\n",
        "        # Convolutions for projecting the global visual vector and the feature maps\n",
        "        self.conv_fs = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_f1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_f2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_f3 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Upsample layers (no parameters, bilinear interpolation)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Final 1x1 convolution for output\n",
        "        self.conv_out = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=1)\n",
        "\n",
        "        # Batch Normalization and ReLU Activation\n",
        "        self.bn = nn.BatchNorm2d(num_features=512)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, F_S, F1, F2, F3):\n",
        "\n",
        "        batch_size = F_S.size(0)\n",
        "\n",
        "        # Reshape F_S to match the spatial dimensions of F3\n",
        "        F_S = F_S.view(batch_size, -1, 1, 1)\n",
        "        F_S = F_S.expand(-1, -1, F3.size(2)//2, F3.size(3)//2)\n",
        "\n",
        "        # Projected Global Visual Vector F_S\n",
        "        F_S_proj = self.relu(self.bn(self.conv_fs(F_S)))\n",
        "\n",
        "        # Projected Feature Maps\n",
        "        F1_proj = self.relu(self.bn(self.conv_f1(F1)))\n",
        "        F2_proj = self.relu(self.bn(self.conv_f2(F2)))\n",
        "        F3_proj = self.relu(self.bn(self.conv_f3(F3)))\n",
        "\n",
        "        # Implementing Feature Pyramid Network\n",
        "        F3_fused = F3_proj + self.upsample(F_S_proj)\n",
        "        F2_fused = F2_proj + self.upsample(F3_fused)\n",
        "        F1_fused = F1_proj + self.upsample(F2_fused)\n",
        "\n",
        "        # Final Output Image Features\n",
        "        F_final = self.conv_out(F1_fused)\n",
        "\n",
        "        return F_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ytNNhMvjqG"
      },
      "source": [
        "**Affordance Head**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-1EA4CtvnQx"
      },
      "outputs": [],
      "source": [
        "class AffordanceHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AffordanceHead, self).__init__()\n",
        "\n",
        "        # ConvTransposed Layers for affordance map generation\n",
        "        self.conv_transpose1 = nn.ConvTranspose2d(1, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_transpose2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_out = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "\n",
        "        # Flatten the Image Features for Matrix Multiplication\n",
        "        B, C, H, W = image_features.size()\n",
        "        image_features = image_features.view(B, C, H * W)\n",
        "\n",
        "        # Compute the Affordance Map (B x H*W)\n",
        "        affordance_map = torch.bmm(text_features.unsqueeze(1), image_features)\n",
        "\n",
        "        # Reshape back to B x 1 x H x W\n",
        "        affordance_map = affordance_map.view(B, 1, H, W)\n",
        "\n",
        "        # Pass through ConvTransposed Layers\n",
        "        #affordance_map = self.relu(self.conv_transpose1(affordance_map))\n",
        "        #affordance_map = self.relu(self.conv_transpose2(affordance_map))\n",
        "\n",
        "        affordance_map = self.relu(self.conv_out(affordance_map))\n",
        "\n",
        "        return affordance_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akwc2Qydvpw2"
      },
      "source": [
        "**AffordanceCLIP Model: Complete Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bujOxkk9vtKe"
      },
      "outputs": [],
      "source": [
        "class AffordanceCLIP(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super(AffordanceCLIP, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.fpn = FeaturePyramidNetwork()\n",
        "        self.affordance_head = AffordanceHead()\n",
        "\n",
        "    def forward(self, images, queries):\n",
        "\n",
        "        # Preprocess images and tokenize queries\n",
        "        text_inputs = clip.tokenize(queries).to(device)\n",
        "        images = images.to(torch.float32)\n",
        "\n",
        "        # Extract Text Features and Image Features\n",
        "        with torch.no_grad():\n",
        "            text_features = self.clip_model.encode_text(text_inputs).to(torch.float32)\n",
        "            image_features = self.clip_model.visual(images).to(torch.float32)\n",
        "\n",
        "\n",
        "        # Extract Hierarchical Feature Maps\n",
        "        features = {}\n",
        "\n",
        "        def get_intermediate_features(name):\n",
        "            def hook(model, input, output):\n",
        "                features[name] = output.to(torch.float32)\n",
        "            return hook\n",
        "\n",
        "        # Register hooks to specific layers\n",
        "        self.clip_model.visual.layer1.register_forward_hook(get_intermediate_features('layer1'))\n",
        "        self.clip_model.visual.layer2.register_forward_hook(get_intermediate_features('layer2'))\n",
        "        self.clip_model.visual.layer3.register_forward_hook(get_intermediate_features('layer3'))\n",
        "\n",
        "        # Forward pass to extract features\n",
        "        with torch.no_grad():\n",
        "            _ = self.clip_model.visual(images)\n",
        "\n",
        "        # Access the intermediate layer outputs\n",
        "        f1 = features['layer1']\n",
        "        f2 = features['layer2']\n",
        "        f3 = features['layer3']\n",
        "\n",
        "        # Pass through the FPN\n",
        "        refined_features = self.fpn(image_features, f1, f2, f3)\n",
        "\n",
        "        # Compute affordance map\n",
        "        affordance_map = self.affordance_head(text_features, refined_features)\n",
        "\n",
        "\n",
        "        return affordance_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZmPOH9Ov8Ht"
      },
      "outputs": [],
      "source": [
        "# Model Creation\n",
        "model = AffordanceCLIP(clip_model).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51jR0_mv8z8"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0jKnYDEv_0E"
      },
      "outputs": [],
      "source": [
        "# Define Binary Cross Entropy\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for batch_idx, (images, queries, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        print(\"Min value in Masks:\", masks.min())\n",
        "        print(\"Max value in Masks:\", masks.max())\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (predict affordance maps)\n",
        "        affordance_maps = model(images, queries)\n",
        "        affordance_maps = torch.sigmoid(affordance_maps)\n",
        "\n",
        "        print(\"Min value in affordance_maps:\", affordance_maps.min())\n",
        "        print(\"Max value in affordance_maps:\", affordance_maps.max())\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(affordance_maps, masks)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss over all batches\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy for the current batch\n",
        "        with torch.no_grad():\n",
        "            predicted = (affordance_maps > 0.5).float()  # Binarize predictions\n",
        "            correct_pixels += (predicted == masks).sum().item()\n",
        "            total_pixels += masks.numel()  # Total number of pixels\n",
        "\n",
        "    # Compute the average loss and accuracy for the epoch\n",
        "    epoch_loss /= len(train_loader)\n",
        "    accuracy = (correct_pixels / total_pixels) * 100\n",
        "\n",
        "    # Print the loss and accuracy for the current epoch\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}