# -*- coding: utf-8 -*-
"""Affordance CLIP Main 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PYawtCFs-8RGZF3Vrc8cw8BfyLo9r2Hx

**Installing OpenAI CLIP**
"""

pip install git+https://github.com/openai/CLIP.git

"""**Import Libraries**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from PIL import Image
import os
import json
import numpy as np
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import clip

"""**Load and Freeze CLIP Model**"""

# Initialize CLIP model and the preprocess function for the images

device = "cuda" if torch.cuda.is_available() else "cpu"

clip_model, preprocess = clip.load("RN101", device=device)

# Freeze CLIP model parameters (no training required)

for param in clip_model.parameters():
    param.requires_grad = False

"""**Loading Affordance Dataset**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Define Paths to Dataset Directories**"""

train_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Images'
train_masks_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Masks'
train_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/annotations.json'

test_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/Images'
test_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/annotations.json'

"""**Load annotations from the JSON file**"""

def load_annotations(annotation_path):
    with open(annotation_path, 'r') as f:
        annotations = json.load(f)
    return annotations

train_annotations = load_annotations(train_annotations_path)
test_annotations = load_annotations(test_annotations_path)

"""**Dataset Class**"""

class AffordanceDataset(Dataset):
    def __init__(self, annotations, images_dir, masks_dir, transform=None):
        self.annotations = annotations
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.transform = transform
        self.preprocess = preprocess

    def load_image(self, image_path):
        image = Image.open(image_path).convert('RGB')
        return image

    def load_mask(self, mask_path):
        mask = Image.open(mask_path).convert('L')
        if self.transform:
            mask = self.transform(mask)
        return mask

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        annotation = self.annotations[idx]
        query = annotation['query']
        object_type = annotation['object_type']
        image_file = annotation['file_name']
        mask_file = annotation['mask_file_name']

        queries = query + " " + object_type

        image_path = os.path.join(self.images_dir, query, object_type, image_file)
        mask_path = os.path.join(self.masks_dir, query, object_type, mask_file)

        image = self.load_image(image_path)
        mask = self.load_mask(mask_path)

        image = self.preprocess(image)

        return image, queries, mask

"""**Train Dataset Creation**"""

transform = transforms.Compose([
    transforms.Resize((58, 58)),
    transforms.ToTensor(),
])

train_dataset = AffordanceDataset(
    annotations=train_annotations,
    images_dir=train_images_dir,
    masks_dir=train_masks_dir,
    transform=transform
)

"""**Create DataLoader for Batching**"""

batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

"""**Feature Pyramid Network (FPN)**"""

class FeaturePyramidNetwork(nn.Module):
    def __init__(self):
        super(FeaturePyramidNetwork, self).__init__()

        # Convolutions for projecting the global visual vector and the feature maps
        self.conv_fs = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)
        self.conv_f1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)
        self.conv_f2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)
        self.conv_f3 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1)

        # Upsample layers (no parameters, bilinear interpolation)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        # Final 1x1 convolution for output
        self.conv_out = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=1)

        # Batch Normalization and ReLU Activation
        self.bn = nn.BatchNorm2d(num_features=512)
        self.relu = nn.ReLU()

    def forward(self, F_S, F1, F2, F3):

        batch_size = F_S.size(0)

        # Reshape F_S to match the spatial dimensions of F3
        F_S = F_S.view(batch_size, -1, 1, 1)
        F_S = F_S.expand(-1, -1, F3.size(2)//2, F3.size(3)//2)

        # Projected Global Visual Vector F_S
        F_S_proj = self.relu(self.bn(self.conv_fs(F_S)))

        # Projected Feature Maps
        F1_proj = self.relu(self.bn(self.conv_f1(F1)))
        F2_proj = self.relu(self.bn(self.conv_f2(F2)))
        F3_proj = self.relu(self.bn(self.conv_f3(F3)))

        # Implementing Feature Pyramid Network
        F3_fused = F3_proj + self.upsample(F_S_proj)
        F2_fused = F2_proj + self.upsample(F3_fused)
        F1_fused = F1_proj + self.upsample(F2_fused)

        # Final Output Image Features
        F_final = self.conv_out(F1_fused)

        return F_final

"""**Affordance Head**"""

class AffordanceHead(nn.Module):
    def __init__(self):
        super(AffordanceHead, self).__init__()

        # ConvTransposed Layers for affordance map generation
        self.conv_transpose1 = nn.ConvTranspose2d(1, 256, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.conv_transpose2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.conv_out = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, text_features, image_features):

        # Flatten the Image Features for Matrix Multiplication
        B, C, H, W = image_features.size()
        image_features = image_features.view(B, C, H * W)

        # Compute the Affordance Map (B x H*W)
        affordance_map = torch.bmm(text_features.unsqueeze(1), image_features)

        # Reshape back to B x 1 x H x W
        affordance_map = affordance_map.view(B, 1, H, W)

        # Pass through ConvTransposed Layers
        #affordance_map = self.relu(self.conv_transpose1(affordance_map))
        #affordance_map = self.relu(self.conv_transpose2(affordance_map))

        # Final Output Layer
        affordance_map = self.relu(self.conv_out(affordance_map))

        return affordance_map

"""**AffordanceCLIP Model: Complete Model**"""

class AffordanceCLIP(nn.Module):
    def __init__(self, clip_model):
        super(AffordanceCLIP, self).__init__()
        self.clip_model = clip_model
        self.fpn = FeaturePyramidNetwork()
        self.affordance_head = AffordanceHead()

    def forward(self, images, queries):

        # Preprocess images and tokenize queries
        text_inputs = clip.tokenize(queries).to(device)
        images = images.to(torch.float32)

        # Extract Text Features and Image Features
        with torch.no_grad():
            text_features = self.clip_model.encode_text(text_inputs).to(torch.float32)
            image_features = self.clip_model.visual(images).to(torch.float32)

        # Extract Hierarchical Feature Maps
        features = {}

        def get_intermediate_features(name):
            def hook(model, input, output):
                features[name] = output.to(torch.float32)
            return hook

        # Register hooks to specific layers
        self.clip_model.visual.layer1.register_forward_hook(get_intermediate_features('layer1'))
        self.clip_model.visual.layer2.register_forward_hook(get_intermediate_features('layer2'))
        self.clip_model.visual.layer3.register_forward_hook(get_intermediate_features('layer3'))

        # Forward pass to extract features
        with torch.no_grad():
            _ = self.clip_model.visual(images)

        # Access the intermediate layer outputs
        f1 = features['layer1']
        f2 = features['layer2']
        f3 = features['layer3']

        # Pass through the FPN
        refined_features = self.fpn(image_features, f1, f2, f3)

        # Compute affordance map
        affordance_map = self.affordance_head(text_features, refined_features)

        return affordance_map

# Model Creation
model = AffordanceCLIP(clip_model).to(device)

"""**Combined BCE + Dice Loss**"""

def dice_loss(predicted, target, eps=1e-10):
    intersection = (predicted * target).sum()
    predicted_sum = predicted.sum()
    target_sum = target.sum()

    dice_score = (2. * intersection + eps) / (predicted_sum + target_sum + eps)  # Dice coefficient
    dice_loss = 1 - dice_score
    return dice_loss

def bce_dice_loss(predicted, target, bce_weight=0.5, eps=1e-10):
    # BCE Loss
    bce_loss = F.binary_cross_entropy(predicted, target)

    # Dice Loss
    dice = dice_loss(predicted, target, eps)

    # Combined loss (you can adjust the weight of each)
    combined_loss = bce_weight * bce_loss + (1 - bce_weight) * dice
    return combined_loss

"""**Evaluation Metrics**"""

# Pixel-wise Accuracy

def pixel_accuracy(predicted, target, tolerance=0.1):
    # Compute total pixels and correct pixels based on a tolerance
    correct_pixels = (torch.abs(predicted - target) < tolerance).sum().item()
    total_pixels = target.numel()

    # Calculate pixel-wise accuracy
    accuracy = correct_pixels / total_pixels
    return accuracy

# Intersection over Union (IoU)

def iou(predicted, target, eps=1e-6):
    intersection = (predicted * target).sum()  # Element-wise multiplication and sum of intersection
    union = predicted.sum() + target.sum() - intersection  # Sum of union (A + B - Intersection)
    iou_score = (intersection + eps) / (union + eps)  # Avoid division by zero with epsilon
    return iou_score.item()

# Dice Coefficient (F1 Score for Segmentation)

def dice_coefficient(predicted, target, eps=1e-10):
    intersection = (predicted * target).sum()  # Element-wise multiplication and sum of intersection
    predicted_sum = predicted.sum()
    target_sum = target.sum()
    dice_score = (2 * intersection + eps) / (predicted_sum + target_sum + eps)  # 2 * Intersection / (A + B)
    return dice_score.item()

"""**Training Loop**"""

# Define Binary Cross Entropy
criterion = nn.BCELoss()

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop
num_epochs = 5

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    total_pixel_accuracy = 0.0
    total_iou = 0.0
    total_dice = 0.0
    total_batches = len(train_loader)

    for batch_idx, (images, queries, masks) in enumerate(train_loader):
        images = images.to(device)
        masks = masks.to(device)

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass (predict affordance maps)
        affordance_maps = model(images, queries)

        # Apply sigmoid to affordance_maps to ensure values are in the range [0, 1]
        affordance_maps = torch.sigmoid(affordance_maps)

         # Compute the BCE + Dice loss
        loss = bce_dice_loss(affordance_maps, masks, bce_weight=0.5)

        # Compute the loss
        # loss = criterion(affordance_maps, masks)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Accumulate the loss
        epoch_loss += loss.item()

        # Detach predicted maps and masks for evaluation (to avoid gradients being tracked)
        affordance_maps = affordance_maps.detach()
        masks = masks.detach()

        # Compute Pixel-wise Accuracy, IoU, and Dice Coefficient
        batch_pixel_accuracy = pixel_accuracy(affordance_maps, masks)
        #batch_iou = iou(affordance_maps, masks)
        #batch_dice = dice_coefficient(affordance_maps, masks)

        # Accumulate metrics
        total_pixel_accuracy += batch_pixel_accuracy
        #total_iou += batch_iou
        #total_dice += batch_dice

    # Compute average loss and metrics for the epoch
    avg_loss = epoch_loss / total_batches
    avg_pixel_accuracy = total_pixel_accuracy / total_batches
    #avg_iou = total_iou / total_batches
    #avg_dice = total_dice / total_batches

    # Print results for the current epoch
    print(f"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {avg_loss:.4f}, Pixel Accuracy: {avg_pixel_accuracy:.4f}")