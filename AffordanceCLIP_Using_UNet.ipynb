{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing OpenAI CLIP**"
      ],
      "metadata": {
        "id": "xdbTIUIidb1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3_FA_ETdaVx"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "M4lfVAO6dgCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import clip"
      ],
      "metadata": {
        "id": "Ceer6YAjdixc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Freeze CLIP Model**"
      ],
      "metadata": {
        "id": "tF7cC9UFdnpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CLIP model and the preprocess function for the images\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model, preprocess = clip.load(\"RN101\", device=device)\n",
        "\n",
        "# Freeze CLIP model parameters (no training required)\n",
        "\n",
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "_I11B9TGdqxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Affordance Dataset**"
      ],
      "metadata": {
        "id": "Rtzq1L-7duMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MAWtYg51dyfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Paths to Dataset Directories**"
      ],
      "metadata": {
        "id": "3yLP_YvXd0yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Images'\n",
        "train_masks_dir = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/Masks'\n",
        "train_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Train_Set/annotations.json'"
      ],
      "metadata": {
        "id": "doRgFRcfd3Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_dir = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/Images'\n",
        "test_annotations_path = '/content/drive/My Drive/AffordanceDatasetMain/Test_Set/annotations.json'"
      ],
      "metadata": {
        "id": "oGF3LW7ed6qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load annotations from the JSON file**"
      ],
      "metadata": {
        "id": "AOqUIePUeCnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_annotations(annotation_path):\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    return annotations\n",
        "\n",
        "train_annotations = load_annotations(train_annotations_path)\n",
        "test_annotations = load_annotations(test_annotations_path)"
      ],
      "metadata": {
        "id": "GGi8UrSpeD1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Dataset Class**"
      ],
      "metadata": {
        "id": "_RyHRsaKeKep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AffordanceDataset(Dataset):\n",
        "    def __init__(self, annotations, images_dir, masks_dir, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def load_mask(self, mask_path):\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform:\n",
        "            mask = self.transform(mask)\n",
        "        return mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        query = annotation['query']\n",
        "        object_type = annotation['object_type']\n",
        "        image_file = annotation['file_name']\n",
        "        mask_file = annotation['mask_file_name']\n",
        "\n",
        "        queries = query + \" \" + object_type\n",
        "\n",
        "        image_path = os.path.join(self.images_dir, query, object_type, image_file)\n",
        "        mask_path = os.path.join(self.masks_dir, query, object_type, mask_file)\n",
        "\n",
        "        image = self.load_image(image_path)\n",
        "        mask = self.load_mask(mask_path)\n",
        "\n",
        "        image = self.preprocess(image)\n",
        "\n",
        "        return image, queries, mask"
      ],
      "metadata": {
        "id": "pH84feNxeSvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Dataset Creation**"
      ],
      "metadata": {
        "id": "PVxbEFhreVB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = AffordanceDataset(\n",
        "    annotations=train_annotations,\n",
        "    images_dir=train_images_dir,\n",
        "    masks_dir=train_masks_dir,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "bYnIIOa1eYzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DataLoader for Batching**"
      ],
      "metadata": {
        "id": "Pbu8k_8fechr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "adCGOBt2eghZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction using CLIP**"
      ],
      "metadata": {
        "id": "eOOr1fjqekch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image, query):\n",
        "\n",
        "    # Preprocess images and tokenize queries\n",
        "    text_inputs = clip.tokenize(query).to(device)\n",
        "\n",
        "    # Extract Text Features and Image Features\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_inputs).to(torch.float32)\n",
        "        image_features = clip_model.encode_image(image).to(torch.float32)\n",
        "        #image_features = self.clip_model.visual(image).to(torch.float32)\n",
        "\n",
        "    return image_features, text_features"
      ],
      "metadata": {
        "id": "9GGu0G-LijUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Segmentation UNet model**"
      ],
      "metadata": {
        "id": "TmJX3C5mkddS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use UNet from segmentation_models_pytorch, with a ResNet-101 Encoder\n",
        "Unet_model = smp.Unet(\n",
        "    encoder_name=\"resnet101\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1\n",
        ").to(device)\n",
        "\n",
        "# Freeze the encoder to keep the pre-trained weights intact\n",
        "for param in Unet_model.encoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "q1c-cDQRsawz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AffordanceCLIP Model: Complete Model**"
      ],
      "metadata": {
        "id": "UAhrHDRA0Cev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetWithCLIP(nn.Module):\n",
        "    def __init__(self, clip_model, unet_model):\n",
        "        super(UNetWithCLIP, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.unet_model = unet_model\n",
        "        self.extract_features = extract_features #\n",
        "        self.fusion_layer = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, image, query, processed_image):\n",
        "\n",
        "        # 1. Extract CLIP features (image and query)\n",
        "        image_features, text_features = self.extract_features(processed_image, query)\n",
        "\n",
        "        # 2. Pass the image through UNet to get the segmentation mask\n",
        "        segmentation_output = self.unet_model(image)\n",
        "\n",
        "        # 3. Fuse CLIP image features with UNet output\n",
        "        image_features = image_features.unsqueeze(2).unsqueeze(3)\n",
        "        image_features = image_features.expand_as(segmentation_output)\n",
        "\n",
        "        concatenated_features = torch.cat((segmentation_output, image_features), dim=1)\n",
        "\n",
        "        # 4. Pass concatenated features through a fusion layer\n",
        "        refined_output = self.fusion_layer(concatenated_features)\n",
        "\n",
        "        # 5. Compute similarity score between image and query features\n",
        "        similarity_score = cosine_similarity(image_features.squeeze(), text_features)\n",
        "\n",
        "        # 6. Use similarity score to modulate the segmentation mask\n",
        "        final_mask = refined_output * similarity_score.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        return final_mask"
      ],
      "metadata": {
        "id": "kE2-gnzErNmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Creation\n",
        "model = UNetWithCLIP(clip_model, unet_model).to(device)"
      ],
      "metadata": {
        "id": "Xjnm6XXM1kZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Binary Cross Entropy\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    total_pixel_accuracy = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_dice = 0.0\n",
        "    total_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, (images, queries, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (predict affordance maps)\n",
        "        affordance_maps = model(images, queries)\n",
        "\n",
        "        # Apply sigmoid to affordance_maps to ensure values are in the range [0, 1]\n",
        "        affordance_maps = torch.sigmoid(affordance_maps)\n",
        "\n",
        "         # Compute the BCE + Dice loss\n",
        "        loss = bce_dice_loss(affordance_maps, masks, bce_weight=0.5)\n",
        "\n",
        "        # Compute the loss\n",
        "        # loss = criterion(affordance_maps, masks)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Detach predicted maps and masks for evaluation (to avoid gradients being tracked)\n",
        "        affordance_maps = affordance_maps.detach()\n",
        "        masks = masks.detach()\n",
        "\n",
        "        # Compute Pixel-wise Accuracy, IoU, and Dice Coefficient\n",
        "        batch_pixel_accuracy = pixel_accuracy(affordance_maps, masks)\n",
        "        batch_iou = iou(affordance_maps, masks)\n",
        "        batch_dice = dice_coefficient(affordance_maps, masks)\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_pixel_accuracy += batch_pixel_accuracy\n",
        "        total_iou += batch_iou\n",
        "        total_dice += batch_dice\n",
        "\n",
        "    # Compute average loss and metrics for the epoch\n",
        "    avg_loss = epoch_loss / total_batches\n",
        "    avg_pixel_accuracy = total_pixel_accuracy / total_batches\n",
        "    avg_iou = total_iou / total_batches\n",
        "    avg_dice = total_dice / total_batches\n",
        "\n",
        "    # Print results for the current epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {avg_loss:.4f}, Pixel Accuracy: {avg_pixel_accuracy:.4f}, IoU: {avg_iou:.4f}, Dice Coefficient: {avg_dice:.4f}\")\n"
      ],
      "metadata": {
        "id": "uFSFYnkR0Od-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}